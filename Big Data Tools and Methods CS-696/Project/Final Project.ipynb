{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Final Project\n",
    "\n",
    "## Team: Tri Dang (814009034), Hsuan Yu Liu (823327369)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Predict the rating of a business based on its reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "1. Load the data\n",
    "2. Clean the data\n",
    "3. Train the model and predict dataset without using PCA\n",
    "4. Train the model and predict dataset using PCA\n",
    "5. Analyze and conclude the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Run\n",
    "\n",
    "1. Install PyEnchant using pip\n",
    "2. Run the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'review.json'\n",
    "outputname = 'smaller_review.json'\n",
    "output_folder = \"output\"\n",
    "download_name = 'clean_review.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from collections import Counter\n",
    "import time\n",
    "import enchant\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import decomposition\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tridang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "program_time = time.time()\n",
    "\n",
    "dictionary = enchant.Dict(\"en_US\")\n",
    "st = LancasterStemmer()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "# what size we want to start with\n",
    "initial_data_size = 10000\n",
    "# size we want to read into Pandas DataFrame. # of Chunks = initial_data_size / chunk_size\n",
    "chunk_size = 100\n",
    "# minimum of words we want in a review \n",
    "min_keep_size = 80\n",
    "# how many features we want to keep\n",
    "pca_components = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing data size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we wanted to use PySpark to clean our data, since the data contained over 6 million rows. We spent an entire day trying to load the data into PySpark DataFrame and perform row transformations to clean our data. However, at the end of the day, we realized that it was a complicated task that could be completed easier in Pandas DataFrame. We were much more familiar with Pandas, and decided that as a team, it was better for us to use Pandas, than to spend much time learning PySpark DataFrame. This would allow us to reduce our code development time. We also understand that using Pandas would also increase our program run time, since it is not suited for large amounts of data. However, it was a tradeoff that we were okay with, since we wanted to be able to quickly develop and test our program. This means that we are only using a small subset of our real dataset, only 10,000 out of 6 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Yelp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a smaller subset of the data to use for development\n",
    "yelp_df = spark.read.json(filename)\n",
    "yelp_df = yelp_df.limit(initial_data_size)\n",
    "\n",
    "fewer_parts = yelp_df.coalesce(1)\n",
    "fewer_parts.write.format('json').save(output_folder)\n",
    "\n",
    "      \n",
    "for filename in os.listdir(output_folder): \n",
    "    src = output_folder + \"/\" + filename\n",
    "    file, file_extension = os.path.splitext(src)\n",
    "    if file_extension == '.json':\n",
    "        os.rename(src, outputname) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we were able to clean the data, we had to load it into Pandas DataFrame. Due to the size of our dataset, the only way we were able to load it efficiently was to load it in chunks. We tested chunksize of 1000 and 100 and found that the smaller chunksize reduced our processing time by 2/3. \n",
    "\n",
    "For the cleaning, we have to take out uncessary characters such as: integers, numerics, single letters (except 'a' and 'i'), quotes, 'markups' (\\n, \\r, etc.), and other symbols. This will leave us with only characters a-z and spaces. After that, we parse the Review Text into tokens, using the NLTK library. \n",
    "\n",
    "Our next task was to reduce our set of tokens into common and similar words. Initially, we only attempted to correct the spelling of the token, without any further processing. However, we were left with a lot of noise and useless/mispelled words that doesn't help our prediction. We then improved our algorithm by first checking to see if each token exist in pre-defined dictionary (en-US). This means that our resulting set of words should only contain correct words. We then lowercase them, and retrieve their \"stem\" using NLTK's LancasterStemmer package, to make sure that past tense and plurals are reduced to its stem word. At the end of this process, we ended up with a much smaller set of features.\n",
    "\n",
    "Once we were able to get a list of all unique words that exists in every reviews, we use that list to crossreference with every review and count the word's frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findwordsandcount(df, review_count):\n",
    "    for index, row in df.T.iteritems():\n",
    "        # keep only alphabetical letters and space\n",
    "        new_string = re.sub(r\"[^a-zA-Z\\ ]+\",\" \", row[\"text\"])\n",
    "        # parse entire review text into tokens\n",
    "        tokens = nltk.word_tokenize(new_string)\n",
    "        # keep only reviews that has more than review_count # of words\n",
    "        df.at[index,'count'] = len(tokens) \n",
    "        if len(tokens) >=review_count:\n",
    "            # for reviews that we want to keep, we apply the removetoken function\n",
    "            df.at[index,'text'] = removetoken(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removetoken(tokens):\n",
    "    save_token =[]\n",
    "    for atoken in tokens:\n",
    "        if (len(atoken) != 1) | (atoken == 'a') | (atoken == 'i') \\\n",
    "        | (atoken == 'A')| (atoken == 'I'):\n",
    "            # we check to see if it's in our dictionary\n",
    "            if (dictionary.check(atoken) == False):\n",
    "                continue\n",
    "            # we lowercase it to make parsing consistent\n",
    "            atoken = str.lower(atoken)\n",
    "            # we get the root word of each token, to make parsing consistent\n",
    "            atoken = st.stem(atoken)\n",
    "            save_token.append(atoken)\n",
    "    return save_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finduniqueword(df):\n",
    "    all_reiew = []\n",
    "    for index, row in df.T.iteritems():\n",
    "        all_reiew  = all_reiew + df.at[index,'text']\n",
    "        all_reiew = list(set(all_reiew))\n",
    "    return all_reiew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countwords(df,unique_word):\n",
    "    dataset_df = pd.DataFrame(columns=['review_id','stars'] + unique_word)\n",
    "\n",
    "    # we count how many times each word appears in each review\n",
    "    for index, a_review in df.iterrows():\n",
    "        empty_row = dict(zip(['review_id','stars'] +unique_word , [0]*(len(unique_word)+2)) )\n",
    "        counts  = Counter(a_review.text)\n",
    "\n",
    "        empty_row['review_id'] = a_review.review_id\n",
    "        empty_row['stars'] = a_review.stars\n",
    "        empty_row.update(counts)\n",
    "        dataset_df.loc[index] = empty_row\n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk:  0\n",
      "findwordsandcount:  0.7892367839813232\n",
      "finduniqueword:  0.010324954986572266\n",
      "countwords:  7.64995002746582\n",
      "Chunk:  1\n",
      "findwordsandcount:  0.6903131008148193\n",
      "finduniqueword:  0.00862574577331543\n",
      "countwords:  7.050591945648193\n",
      "Chunk:  2\n",
      "findwordsandcount:  0.6202528476715088\n",
      "finduniqueword:  0.006911039352416992\n",
      "countwords:  5.700318098068237\n",
      "Chunk:  3\n",
      "findwordsandcount:  0.6181550025939941\n",
      "finduniqueword:  0.006780862808227539\n",
      "countwords:  5.908515930175781\n",
      "Chunk:  4\n",
      "findwordsandcount:  0.6343197822570801\n",
      "finduniqueword:  0.011677026748657227\n",
      "countwords:  7.1894121170043945\n",
      "Chunk:  5\n",
      "findwordsandcount:  0.5121910572052002\n",
      "finduniqueword:  0.009585142135620117\n",
      "countwords:  5.58926796913147\n",
      "Chunk:  6\n",
      "findwordsandcount:  0.6115961074829102\n",
      "finduniqueword:  0.00706791877746582\n",
      "countwords:  6.22661566734314\n",
      "Chunk:  7\n",
      "findwordsandcount:  0.6816027164459229\n",
      "finduniqueword:  0.008348941802978516\n",
      "countwords:  7.212285041809082\n",
      "Chunk:  8\n",
      "findwordsandcount:  0.5819821357727051\n",
      "finduniqueword:  0.009234905242919922\n",
      "countwords:  5.700730085372925\n",
      "Chunk:  9\n",
      "findwordsandcount:  0.7018160820007324\n",
      "finduniqueword:  0.008938074111938477\n",
      "countwords:  8.19193410873413\n",
      "Chunk:  10\n",
      "findwordsandcount:  0.6250061988830566\n",
      "finduniqueword:  0.010977983474731445\n",
      "countwords:  6.935883045196533\n",
      "Chunk:  11\n",
      "findwordsandcount:  0.508915901184082\n",
      "finduniqueword:  0.0065762996673583984\n",
      "countwords:  5.728622198104858\n",
      "Chunk:  12\n",
      "findwordsandcount:  0.5852041244506836\n",
      "finduniqueword:  0.007582902908325195\n",
      "countwords:  6.343376874923706\n",
      "Chunk:  13\n",
      "findwordsandcount:  0.567331075668335\n",
      "finduniqueword:  0.008347749710083008\n",
      "countwords:  6.9175379276275635\n",
      "Chunk:  14\n",
      "findwordsandcount:  0.5437371730804443\n",
      "finduniqueword:  0.006058931350708008\n",
      "countwords:  5.661776781082153\n",
      "Chunk:  15\n",
      "findwordsandcount:  0.6682188510894775\n",
      "finduniqueword:  0.01118779182434082\n",
      "countwords:  9.258312940597534\n",
      "Chunk:  16\n",
      "findwordsandcount:  0.595238208770752\n",
      "finduniqueword:  0.010887384414672852\n",
      "countwords:  7.367166996002197\n",
      "Chunk:  17\n",
      "findwordsandcount:  0.5686929225921631\n",
      "finduniqueword:  0.0071260929107666016\n",
      "countwords:  6.063592910766602\n",
      "Chunk:  18\n",
      "findwordsandcount:  0.5583958625793457\n",
      "finduniqueword:  0.009467124938964844\n",
      "countwords:  7.320309162139893\n",
      "Chunk:  19\n",
      "findwordsandcount:  0.5336718559265137\n",
      "finduniqueword:  0.0074808597564697266\n",
      "countwords:  6.757017135620117\n",
      "Chunk:  20\n",
      "findwordsandcount:  0.5658090114593506\n",
      "finduniqueword:  0.0074100494384765625\n",
      "countwords:  6.656949996948242\n",
      "Chunk:  21\n",
      "findwordsandcount:  0.6375699043273926\n",
      "finduniqueword:  0.008642196655273438\n",
      "countwords:  8.534101963043213\n",
      "Chunk:  22\n",
      "findwordsandcount:  0.4824860095977783\n",
      "finduniqueword:  0.00958108901977539\n",
      "countwords:  5.982553243637085\n",
      "Chunk:  23\n",
      "findwordsandcount:  0.39304018020629883\n",
      "finduniqueword:  0.004909038543701172\n"
     ]
    }
   ],
   "source": [
    "words_list = []\n",
    "cols = ['review_id','stars','text']\n",
    "\n",
    "all_dataset_df = pd.DataFrame(columns=['review_id','stars'])\n",
    "\n",
    "# record the time\n",
    "start_time = time.time()\n",
    "clean_time = time.time()\n",
    "chunk_count = 0\n",
    "\n",
    "for part_df in pd.read_json(outputname, lines=True, chunksize=chunk_size):\n",
    "    part_df = part_df[cols]\n",
    "    part_df['count']  = 0\n",
    "    findwordsandcount(part_df, min_keep_size)\n",
    "    print (\"Chunk: \", chunk_count)\n",
    "    print(\"findwordsandcount: \", (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    part_df = part_df.loc[part_df['count'] >= min_keep_size]\n",
    "    unique_word = finduniqueword(part_df)\n",
    "    unique_word.sort()\n",
    "    \n",
    "    print(\"finduniqueword: \", (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "\n",
    "    dataset_df = countwords(part_df,unique_word)\n",
    "    \n",
    "    all_dataset_df = all_dataset_df.append(dataset_df,sort=False)\n",
    "    print(\"countwords: \", (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    chunk_count+=1\n",
    "\n",
    "print(\"Cleaning time: \", (time.time() - clean_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling NaN with 0s\n",
    "all_dataset_df = all_dataset_df.fillna(0)\n",
    "all_dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset_df.to_csv(download_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model and predict dataset without using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset_df = pd.read_csv(download_name)\n",
    "columns_word_only = all_dataset_df.columns.drop(['stars','review_id'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\\\n",
    "    all_dataset_df[columns_word_only], \\\n",
    "    all_dataset_df[\"stars\"], test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_value in range(4,20,2):\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k_value)\n",
    "    knn_model.fit(x_train, y_train)\n",
    "    knn_y_predict = knn_model.predict(x_test)\n",
    "    print(\"K value: \", k_value, \"   Accuracy: \", accuracy_score(y_test, knn_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=14)\n",
    "knn_model.fit(x_train, y_train)\n",
    "knn_y_predict = knn_model.predict(x_test)\n",
    "print(\"K nearest neighbor without PCA\")\n",
    "print(confusion_matrix(y_test, knn_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the model and predict dataset using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_word_only = all_dataset_df.columns.drop(['stars','review_id'])\n",
    "\n",
    "pca = decomposition.PCA(n_components= pca_components)\n",
    "principalComponents = pca.fit_transform(all_dataset_df[columns_word_only])\n",
    "pca_x_Df = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\\\n",
    "    pca_x_Df, all_dataset_df[\"stars\"], test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K nearest neighbor (KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_value in range(4,20,2):\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k_value)\n",
    "    knn_model.fit(x_train, y_train)\n",
    "    knn_y_predict = knn_model.predict(x_test)\n",
    "    print(\"K value: \", k_value, \"   Accuracy: \", accuracy_score(y_test, knn_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=8)\n",
    "knn_model.fit(x_train, y_train)\n",
    "knn_y_predict = knn_model.predict(x_test)\n",
    "print(\"K nearest neighbor(with PCA)\")\n",
    "print(confusion_matrix(y_test, knn_y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze & Conclude the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use K-NearestNeighbor model, the highest accuracy we obtained was when K = 8, for both with PCA and without PCA. This means that we can reliably use a smaller subset of features, instead of having to use all of the features, reducing our run time by a substantial amount. \n",
    "\n",
    "Because there were 6 labels (ratings), the random select probability for correct prediction is 1/6. Using our algorithm, we were able to make a prediction with an accuracy of approximately 2/5. Although it is not entirely accurate, it is a noticable mark in improvement on random selection. We predict that if we were to use the entire 6 million reviews, our accuracy will improve."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
